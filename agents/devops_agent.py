"""
DevOps Agent —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AutoGen –∏ Ollama
–ê–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –¥–µ–ø–ª–æ—è
"""

from autogen import AssistantAgent, UserProxyAgent

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM –¥–ª—è Ollama
OLLAMA_CONFIG = {
    "model": "qwen2.5:7b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "ollama",
    "api_type": "open_ai"
}

# –ê–≥–µ–Ω—Ç-–∫–æ–¥–µ—Ä
coder = AssistantAgent(
    name="Coder",
    system_message="–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –ü–∏—à–µ—à—å —á–∏—Å—Ç—ã–π, –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥. "
                   "–°–ª–µ–¥—É–µ—à—å best practices –∏ —Å–æ–∑–¥–∞–µ—à—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è.",
    llm_config=OLLAMA_CONFIG
)

# –ê–≥–µ–Ω—Ç-—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫
tester = AssistantAgent(
    name="Tester",
    system_message="–¢—ã QA –∏–Ω–∂–µ–Ω–µ—Ä. –°–æ–∑–¥–∞–µ—à—å —Ç–µ—Å—Ç—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—à—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞. "
                   "–ü–∏—à–µ—à—å unit-—Ç–µ—Å—Ç—ã, integration-—Ç–µ—Å—Ç—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—à—å –ø–æ–∫—Ä—ã—Ç–∏–µ –∫–æ–¥–∞.",
    llm_config=OLLAMA_CONFIG
)

# –ê–≥–µ–Ω—Ç-–¥–µ–ø–ª–æ–µ—Ä
deployer = AssistantAgent(
    name="Deployer",
    system_message="–¢—ã DevOps –∏–Ω–∂–µ–Ω–µ—Ä. –°–æ–∑–¥–∞–µ—à—å Kubernetes –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã, Dockerfiles, "
                   "–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—à—å CI/CD –∏ –≥–æ—Ç–æ–≤–∏—à—å –¥–µ–ø–ª–æ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.",
    llm_config=OLLAMA_CONFIG
)

# –ê–≥–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ Ollama
model_manager = AssistantAgent(
    name="ModelManager",
    system_message="""–¢—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –º–æ–¥–µ–ª—è–º–∏ Ollama.

–¢–≤–æ–∏ –∑–∞–¥–∞—á–∏:
- –ó–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥—É ollama pull
- –ü—Ä–æ–≤–µ—Ä—è—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ ollama list
- –£–ø—Ä–∞–≤–ª—è—Ç—å –≤–µ—Ä—Å–∏—è–º–∏ –º–æ–¥–µ–ª–µ–π
- –ü—Ä–æ–≤–µ—Ä—è—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
- –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á

–ö–æ–º–∞–Ω–¥—ã Ollama:
- ollama pull <model> - –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
- ollama list - –ø–æ–∫–∞–∑–∞—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
- ollama show <model> - –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
- ollama run <model> - –∑–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å

–ü—Ä–∏–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π:
- qwen2.5:7b - –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- qwen:32b - –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å (—Ç—Ä–µ–±—É–µ—Ç >= 20GB RAM)
- llama3:8b - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
- mistral:7b - –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å

–í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π –¥–æ—Å—Ç—É–ø–Ω–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π.
""",
    llm_config=OLLAMA_CONFIG
)

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–≥–µ–Ω—Ç
user = UserProxyAgent(
    name="User",
    human_input_mode="NEVER",  # –î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
    max_consecutive_auto_reply=10,
    code_execution_config={
        "work_dir": ".",
        "use_docker": False
    }
)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üöÄ DevOps Agent –∑–∞–ø—É—â–µ–Ω!")
    print("üìù –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:")
    print("   - coder: –¥–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞")
    print("   - tester: –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤")
    print("   - deployer: –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–µ–ø–ª–æ—è")
    print("   - model_manager: –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ Ollama")
    print("   - user: –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∞–≥–µ–Ω—Ç–∞–º–∏")
    print("\n–ü—Ä–∏–º–µ—Ä—ã:")
    print("  user.initiate_chat(coder, message='–°–æ–∑–¥–∞–π –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏')")
    print("  user.initiate_chat(model_manager, message='–ó–∞–≥—Ä—É–∑–∏ –º–æ–¥–µ–ª—å qwen:32b')")
